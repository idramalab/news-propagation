{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tldextract\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_domain(url):\n",
    "    return tldextract.extract(url).registered_domain\n",
    "\n",
    "BASE_DIR = '/data/spirits-backup/savvas/news-propagation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabdf = pd.read_csv(BASE_DIR + 'gab_hits_news_propagation.txt', delimiter='\\t')\n",
    "chandf = pd.read_csv(BASE_DIR + '4chan_news_propagation_hits.txt', delimiter='\\t')\n",
    " \n",
    "def load_reddit(directory):\n",
    "    l = []\n",
    "    with open(directory, 'r') as f:\n",
    "        for line in f:\n",
    "            els = line.replace('\\n', '').split('\\t')\n",
    "            if len(els)>7:\n",
    "                # means the line refers to a comment\n",
    "                item = {'url': els[0], 'author': els[1], 'created_at': els[2], 'subreddit': els[3], 'score': els[4],\n",
    "                       'id': els[5], 'link_id': els[6], 'gilded':els[7], 'controversiality': els[8]}\n",
    "            else:\n",
    "                # means the line refers to a submission\n",
    "                item = {'url': els[0], 'author': els[1], 'created_at': els[2], 'subreddit': els[3], 'num_comments': els[4],\n",
    "                       'score': els[5], 'id': els[6]}\n",
    "            l.append(item)\n",
    "    return pd.DataFrame(l)\n",
    "redditdf = load_reddit(BASE_DIR + 'reddit_news_propagation_hits.txt')\n",
    "redditdf = redditdf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domains that are alternative = 37. Domains that are mainstream = 1036\n"
     ]
    }
   ],
   "source": [
    "newsguard_dict = json.load(open(BASE_DIR + 'list_of_top_30000_newswebsites_on_newsguard.json', 'r'))\n",
    "count_mainstream = 0\n",
    "for k,v in newsguard_dict.items():\n",
    "    if v>=60.0:\n",
    "        count_mainstream+=1\n",
    "        \n",
    "count_alternative = len(newsguard_dict.items()) - count_mainstream\n",
    "print(\"Domains that are alternative = %d. Domains that are mainstream = %d\" %(count_alternative, count_mainstream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_data_twitter(domains):\n",
    "    data = []\n",
    "    all_files = glob.glob(\"/data/savvas/scylla_fastdata/news-propagation/historic_news_tweets_gardenhose/*.txt\")\n",
    "    for inp_file in all_files:\n",
    "        with open(inp_file, 'r') as inp:\n",
    "            for line in inp:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    #print(type(tweet))\n",
    "                    for url in tweet['entities']['urls']:\n",
    "                        url = url['expanded_url']\n",
    "                        domain = extract_domain(url)\n",
    "                        if domain in domains:\n",
    "                            filtered_dict = {}\n",
    "                            filtered_dict['created_utc'] = int(tweet['timestamp_ms'])/float(1000)\n",
    "                            filtered_dict['url'] = url\n",
    "                            #filtered_dict['retweeted'] = tweet['retweeted']\n",
    "                            #filtered_dict['text'] = tweet['text']\n",
    "                            filtered_dict['tweet_id'] = tweet['id_str']\n",
    "                            #filtered_dict['filter_level'] = tweet['filter_level']\n",
    "                            #filtered_dict['favorited'] = tweet['favorited']\n",
    "                            filtered_dict['user'] = tweet['user']['screen_name']\n",
    "                            filtered_dict['user_id'] = tweet['user']['id_str']\n",
    "                            filtered_dict['user_followers'] = tweet['user']['followers_count']\n",
    "                            filtered_dict['user_friends'] = tweet['user']['friends_count']\n",
    "                            #filtered_dict['user_favourites'] = tweet['user']['statuses_count']\n",
    "                            #filtered_dict['user_listed_count'] = tweet['user']['listed_count']\n",
    "                            filtered_dict['created_utc_friendly'] = tweet['created_at']\n",
    "                            #filtered_dict['retweet_count'] = tweet['retweet_count']\n",
    "                            #filtered_dict['favorite_count'] = tweet['favorite_count']\n",
    "                            filtered_dict['lang'] = tweet['lang']\n",
    "                            data.append(filtered_dict)\n",
    "                except Exception as e: \n",
    "                    #print(str(e))\n",
    "                    pass\n",
    "            #print(\"Done with %s. Added = %d\" %(inp_file, len(data)))\n",
    "    return pd.DataFrame(data)\n",
    "                          \n",
    "twitterdf = read_data_twitter(newsguard_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditdf['datetime'] = pd.to_datetime(redditdf['created_at'], unit='s', utc=True)\n",
    "gabdf['datetime'] = pd.to_datetime(gabdf['published_at'], utc=True)\n",
    "chandf['datetime'] = pd.to_datetime(chandf['published_at'], unit='s', utc=True)\n",
    "twitterdf['datetime'] = pd.to_datetime(twitterdf['created_utc'], unit='s', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tddf = redditdf[redditdf.subreddit=='The_Donald']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tddf = redditdf[redditdf.subreddit=='The_Donald']\n",
    "\n",
    "dfs = [gabdf, chandf, redditdf, twitterdf, tddf]\n",
    "for df in dfs:\n",
    "    df['domain'] = df['url'].map(extract_domain)\n",
    "for df in dfs:\n",
    "    df['newsguard_score'] = df['domain'].map(newsguard_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform = Twitter Mainstream Count = 7275627 Mainstream Percentage = 91.296670 Alternative Count = 693587 Alternative Percentage = 8.703330\n",
      "Mainstream domains Percentage = 95.992544\n",
      "Platform = Reddit Mainstream Count = 26232779 Mainstream Percentage = 94.969592 Alternative Count = 1389514 Alternative Percentage = 5.030408\n",
      "Mainstream domains Percentage = 96.085741\n",
      "Platform = The_Donald Mainstream Count = 570968 Mainstream Percentage = 74.274096 Alternative Count = 197763 Alternative Percentage = 25.725904\n",
      "Mainstream domains Percentage = 95.526561\n",
      "Platform = 4chan Mainstream Count = 591568 Mainstream Percentage = 87.395127 Alternative Count = 85321 Alternative Percentage = 12.604873\n",
      "Mainstream domains Percentage = 94.874185\n",
      "Platform = Gab Mainstream Count = 2382626 Mainstream Percentage = 51.211372 Alternative Count = 2269907 Alternative Percentage = 48.788628\n",
      "Mainstream domains Percentage = 95.899348\n",
      "All posts from all platforms = 37822312\n",
      "All URLs from all platform = 15621263\n",
      "All mainstream URLs = 14636451\n",
      "All alternative URLs = 984812\n",
      "All mainstream posts = 33556092\n",
      "All alternative posts = 4369923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Web community</th>\n",
       "      <th># of posts (mainstream)</th>\n",
       "      <th># of posts (alternative)</th>\n",
       "      <th># unique URLs (mainstream)</th>\n",
       "      <th># of unique URLs (alternative)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>7123715</td>\n",
       "      <td>686497</td>\n",
       "      <td>3893357</td>\n",
       "      <td>291354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>23605406</td>\n",
       "      <td>1342429</td>\n",
       "      <td>11170005</td>\n",
       "      <td>612213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>528142</td>\n",
       "      <td>190742</td>\n",
       "      <td>385384</td>\n",
       "      <td>122204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4chan</td>\n",
       "      <td>458431</td>\n",
       "      <td>75705</td>\n",
       "      <td>275422</td>\n",
       "      <td>37472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gab</td>\n",
       "      <td>2369149</td>\n",
       "      <td>2265336</td>\n",
       "      <td>749547</td>\n",
       "      <td>385317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Web community  # of posts (mainstream)  # of posts (alternative)  \\\n",
       "0       Twitter                  7123715                    686497   \n",
       "1        Reddit                 23605406                   1342429   \n",
       "2    The_Donald                   528142                    190742   \n",
       "3         4chan                   458431                     75705   \n",
       "4           Gab                  2369149                   2265336   \n",
       "\n",
       "   # unique URLs (mainstream)  # of unique URLs (alternative)  \n",
       "0                     3893357                          291354  \n",
       "1                    11170005                          612213  \n",
       "2                      385384                          122204  \n",
       "3                      275422                           37472  \n",
       "4                      749547                          385317  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [twitterdf, redditdf, tddf, chandf, gabdf]\n",
    "dfs_names = ['Twitter', 'Reddit', 'The_Donald', '4chan', 'Gab' ]\n",
    "keys = ['tweet_id', 'id', 'id', 'post_id', 'post_id']\n",
    "def generate_dataset_table(dfs, dfs_names):\n",
    "    out = []\n",
    "    all_ids = []\n",
    "    all_urls = []\n",
    "    all_mainstream = []\n",
    "    all_alternative = []\n",
    "    all_mainstream_ids = []\n",
    "    all_alternative_ids = [] \n",
    "    for i in range(len(dfs)):\n",
    "        platform = dfs_names[i]\n",
    "        df = dfs[i]\n",
    "        df = df[df.datetime<'2018-11-01']\n",
    "        all_ids.append(df[keys[i]].tolist())\n",
    "        all_urls.append(df['url'].tolist())\n",
    "        mainstreamdf = df[df.newsguard_score>=60]\n",
    "        alternativedf = df[df.newsguard_score<60]\n",
    "        print(\"Platform = %s Mainstream Count = %d Mainstream Percentage = %f Alternative Count = %d Alternative Percentage = %f\" \n",
    "             %(platform, mainstreamdf.shape[0], mainstreamdf.shape[0]/(mainstreamdf.shape[0]+alternativedf.shape[0])*100,\n",
    "              alternativedf.shape[0], alternativedf.shape[0]/(mainstreamdf.shape[0]+alternativedf.shape[0])*100))\n",
    "        print(\"Mainstream domains Percentage = %f\" %(len(set(mainstreamdf['domain'].tolist()))/1073*100))\n",
    "        mainstream_count = len(set(mainstreamdf[keys[i]].tolist()))\n",
    "        alternative_count = len(set(alternativedf[keys[i]].tolist()))\n",
    "        \n",
    "        all_mainstream_ids.append(mainstreamdf[keys[i]].tolist())\n",
    "        all_alternative_ids.append(alternativedf[keys[i]].tolist())\n",
    "\n",
    "        mainstream_unique_urls = len(set(mainstreamdf['url'].tolist()))\n",
    "        alternative_unique_urls = len(set(alternativedf['url'].tolist()))\n",
    "        all_mainstream.append(mainstreamdf['url'].tolist())\n",
    "        all_alternative.append(alternativedf['url'].tolist())\n",
    "        \n",
    "        out.append({'Web community': platform, '# of posts (mainstream)': mainstream_count, \n",
    "                   '# of posts (alternative)': alternative_count,\n",
    "                   '# unique URLs (mainstream)': mainstream_unique_urls, \n",
    "                   '# of unique URLs (alternative)': alternative_unique_urls})\n",
    "    flat_ids = [item for sublist in all_ids for item in sublist]\n",
    "    flat_urls = [item for sublist in all_urls for item in sublist]\n",
    "    flat_mainstream = [item for sublist in all_mainstream for item in sublist]\n",
    "    flat_alternative = [item for sublist in all_alternative for item in sublist]\n",
    "    \n",
    "    flat_mainstream_ids = [item for sublist in all_mainstream_ids for item in sublist]\n",
    "    flat_alternative_ids = [item for sublist in all_alternative_ids for item in sublist]\n",
    "\n",
    "    print(\"All posts from all platforms = %d\" %(len(set(flat_ids))))\n",
    "    print(\"All URLs from all platform = %d\" %(len(set(flat_urls))))\n",
    "    print(\"All mainstream URLs = %d\" %(len(set(flat_mainstream))))\n",
    "    print(\"All alternative URLs = %d\" %(len(set(flat_alternative))))\n",
    "    print(\"All mainstream posts = %d\" %(len(set(flat_mainstream_ids))))\n",
    "    print(\"All alternative posts = %d\" %(len(set(flat_alternative_ids))))\n",
    "    return pd.DataFrame(out), list(set(flat_urls))\n",
    "\n",
    "dataset_table, all_urls_all_platforms = generate_dataset_table(dfs, dfs_names)\n",
    "dataset_table = dataset_table[['Web community', '# of posts (mainstream)', '# of posts (alternative)',\n",
    "                              '# unique URLs (mainstream)','# of unique URLs (alternative)' ]]\n",
    "dataset_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_types_blacklist = ['TIME', 'DATE', 'PERCENT', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "def load_entity_mapping_multifile(file_path, id_key):\n",
    "    d = {}\n",
    "    for filename in os.listdir(file_path):\n",
    "        with open(file_path + filename, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                post_id = data[id_key]\n",
    "                entities = []\n",
    "                list_json = data['entities']\n",
    "                for ent in list_json:\n",
    "                    if len(ent['entity_text'])>1 and ent['entity_label'] not in entities_types_blacklist and not ent['entity_text'].startswith('RT'):\n",
    "                        entities.append(ent['entity_text'])\n",
    "                d[post_id] = entities\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_entity_mapping(filename, id_key):\n",
    "    d = {}\n",
    "    entities_blacklist = ['LRB', 'RRB']\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            post_id = data[id_key]\n",
    "            entities = []\n",
    "            list_json = data['entities']\n",
    "            for ent in list_json:\n",
    "                if len(ent['entity_text'])>1 and ent['entity_label'] not in entities_types_blacklist and not ent['entity_text'].startswith('RT'):\n",
    "                    if ent['entity_text'] not in entities_blacklist:\n",
    "                        entities.append(ent['entity_text'])\n",
    "            d[post_id] = entities\n",
    "    return d\n",
    "            \n",
    "\n",
    "gab_entities_dict = load_entity_mapping(BASE_DIR + 'gab_entity_detection_output_truecase_mp.txt', 'post_id')\n",
    "chan_entities_dict = load_entity_mapping_multifile(BASE_DIR + 'entities_results/4chan/4chan_entities/', 'post_id')\n",
    "reddit_entities_dict = load_entity_mapping_multifile(BASE_DIR + 'entities_results/reddit_specific/reddit_discussions_entities_specific/', 'post_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_post(post_id, data_dict):\n",
    "    try:\n",
    "        return data_dict[post_id]\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "redditdf['entities_list'] = redditdf['id'].apply(map_entities_to_post, args=(reddit_entities_dict,))\n",
    "chandf['entities_list'] = chandf['post_id'].apply(map_entities_to_post, args=(chan_entities_dict,))\n",
    "gabdf['entities_list'] = gabdf['post_id'].apply(map_entities_to_post, args=(gab_entities_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_entities_dict = load_entity_mapping('/data/savvas/scylla_fastdata/news-propagation/all_tweets_entities.txt', 'post_id')\n",
    "twitterdf['entities_list'] = twitterdf['tweet_id'].astype(int).apply(map_entities_to_post, args=(twitter_entities_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls_dict = {}\n",
    "for url in all_urls_all_platforms:\n",
    "    all_urls_dict[url] = 1\n",
    "\n",
    "def url_exists_in_platforms(url):\n",
    "    try:\n",
    "        a = all_urls_dict[url]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "article_entities_dir = '/data/savvas/scylla_fastdata/news-propagation/articles_text_entities/'\n",
    "entities_types_blacklist = ['TIME', 'DATE', 'PERCENT', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "def load_article_entities_to_dict(directory):\n",
    "    output_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(directory + filename, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                url = data['article_url']\n",
    "                if not url_exists_in_platforms(url):\n",
    "                    continue\n",
    "                entities = data['entities']\n",
    "                output_entities = []\n",
    "                for ent in entities:\n",
    "                    t = ent['entity_label']\n",
    "                    if t not in entities_types_blacklist and ent['entity_text']!='â€™m':\n",
    "                        output_entities.append(ent['entity_text'])\n",
    "                output_dict[url] = list(set(output_entities))\n",
    "    return output_dict\n",
    "\n",
    "articles_entities_dict = load_article_entities_to_dict(article_entities_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_top_entities_from_articles(articles_dict, topn):\n",
    "    all_posts = len(articles_dict.items())\n",
    "    list_of_lists_entities = list(articles_dict.values())\n",
    "    flat_list = [item for sublist in list_of_lists_entities for item in sublist]\n",
    "    counter = Counter(flat_list).most_common(topn)\n",
    "    res = []\n",
    "    for c in counter:\n",
    "        res.append({'Entity': c[0], 'Percentage (%)': c[1]/all_posts*100})\n",
    "    return pd.DataFrame(res)\n",
    "    #return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Percentage (%)</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Percentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump</td>\n",
       "      <td>17.945819</td>\n",
       "      <td>Trump</td>\n",
       "      <td>27.524544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>15.533495</td>\n",
       "      <td>US</td>\n",
       "      <td>18.740361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American</td>\n",
       "      <td>11.156335</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>18.301195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>11.086959</td>\n",
       "      <td>American</td>\n",
       "      <td>15.548007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the United States</td>\n",
       "      <td>10.135122</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>13.767639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Republican</td>\n",
       "      <td>9.033982</td>\n",
       "      <td>Russia</td>\n",
       "      <td>13.399152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Washington</td>\n",
       "      <td>8.441719</td>\n",
       "      <td>the United States</td>\n",
       "      <td>13.145026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>America</td>\n",
       "      <td>7.242032</td>\n",
       "      <td>America</td>\n",
       "      <td>11.540240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New York</td>\n",
       "      <td>6.879390</td>\n",
       "      <td>Russian</td>\n",
       "      <td>11.145287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Americans</td>\n",
       "      <td>6.759449</td>\n",
       "      <td>Obama</td>\n",
       "      <td>10.206797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>6.524556</td>\n",
       "      <td>Americans</td>\n",
       "      <td>9.779853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Congress</td>\n",
       "      <td>6.189344</td>\n",
       "      <td>Republican</td>\n",
       "      <td>9.503621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Republicans</td>\n",
       "      <td>6.115076</td>\n",
       "      <td>Washington</td>\n",
       "      <td>9.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Obama</td>\n",
       "      <td>5.988409</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>8.815005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>US</td>\n",
       "      <td>5.965562</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>8.441947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Democratic</td>\n",
       "      <td>5.903130</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>7.480287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Democrats</td>\n",
       "      <td>5.789807</td>\n",
       "      <td>Congress</td>\n",
       "      <td>7.462537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Russia</td>\n",
       "      <td>5.738030</td>\n",
       "      <td>Republicans</td>\n",
       "      <td>6.958750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>5.719234</td>\n",
       "      <td>Syria</td>\n",
       "      <td>6.730239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>5.536270</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>6.660091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Entity  Percentage (%)             Entity  Percentage (%)\n",
       "0               Trump       17.945819              Trump       27.524544\n",
       "1                U.S.       15.533495                 US       18.740361\n",
       "2            American       11.156335       Donald Trump       18.301195\n",
       "3        Donald Trump       11.086959           American       15.548007\n",
       "4   the United States       10.135122               U.S.       13.767639\n",
       "5          Republican        9.033982             Russia       13.399152\n",
       "6          Washington        8.441719  the United States       13.145026\n",
       "7             America        7.242032            America       11.540240\n",
       "8            New York        6.879390            Russian       11.145287\n",
       "9           Americans        6.759449              Obama       10.206797\n",
       "10            Reuters        6.524556          Americans        9.779853\n",
       "11           Congress        6.189344         Republican        9.503621\n",
       "12        Republicans        6.115076         Washington        9.264800\n",
       "13              Obama        5.988409          Democrats        8.815005\n",
       "14                 US        5.965562           Facebook        8.441947\n",
       "15         Democratic        5.903130    Hillary Clinton        7.480287\n",
       "16          Democrats        5.789807           Congress        7.462537\n",
       "17             Russia        5.738030        Republicans        6.958750\n",
       "18           Facebook        5.719234              Syria        6.730239\n",
       "19            Twitter        5.536270            Twitter        6.660091"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_entities_articles_mainstream_alternative(articles_dict, topn):\n",
    "    all_posts = len(articles_dict.items())\n",
    "    mainstream_urls = []\n",
    "    alternative_urls = []\n",
    "    for k,v in articles_dict.items():\n",
    "        d = extract_domain(k)\n",
    "        try:\n",
    "            score = newsguard_dict[d]\n",
    "        except KeyError:\n",
    "            #print(d)\n",
    "            continue\n",
    "        if score<60.0:\n",
    "            alternative_urls.append(k)\n",
    "        else:\n",
    "            mainstream_urls.append(k)\n",
    "            \n",
    "    mainstream_urls = list(set(mainstream_urls))\n",
    "    alternative_urls = list(set(alternative_urls))\n",
    "    \n",
    "    \n",
    "    mainstream_entities = []\n",
    "    alternative_entities = []\n",
    "    for u in mainstream_urls:\n",
    "        mainstream_entities.append(list(set(articles_dict[u])))\n",
    "    for u in alternative_urls:\n",
    "        alternative_entities.append(list(set(articles_dict[u])))\n",
    "        \n",
    "    flat_list_mainstream = [item for sublist in mainstream_entities for item in sublist]\n",
    "    counter = Counter(flat_list_mainstream).most_common(topn)\n",
    "    res = []\n",
    "    for c in counter:\n",
    "        res.append({'Entity': c[0], 'Percentage (%)': c[1]/len(mainstream_urls)*100})\n",
    "    df_mainstream = pd.DataFrame(res)\n",
    "    \n",
    "    flat_list_alternative = [item for sublist in alternative_entities for item in sublist]\n",
    "    counter = Counter(flat_list_alternative).most_common(topn)\n",
    "    res = []\n",
    "    for c in counter:\n",
    "        res.append({'Entity': c[0], 'Percentage (%)': c[1]/len(alternative_urls)*100})\n",
    "    df_alternative = pd.DataFrame(res)\n",
    "    return pd.concat([df_mainstream, df_alternative], axis=1)\n",
    "\n",
    "top_entities_articles_m_a = top_entities_articles_mainstream_alternative(articles_entities_dict, 20)\n",
    "top_entities_articles_m_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [twitterdf, redditdf, tddf, chandf, gabdf]\n",
    "dfs_names = ['Twitter', 'Reddit', 'The_Donald', '4chan', 'Gab' ]\n",
    "unique_ids_keys = ['tweet_id', 'id', 'id', 'post_id', 'post_id']\n",
    "\n",
    "entities_lengths = []\n",
    "for df in dfs:\n",
    "    df['entities_len'] = df['entities_list'].map(len)\n",
    "    entities_lengths.append(df['entities_len'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [twitterdf, redditdf, tddf, chandf, gabdf]\n",
    "dfs_names = ['Twitter', 'Reddit', 'The_Donald', '4chan', 'Gab' ]\n",
    "unique_ids_keys = ['tweet_id', 'id', 'id', 'post_id', 'post_id']\n",
    "def find_top_entities(dfs, dfs_names, topn):\n",
    "    output_dfs = []\n",
    "    top_entities_all = []\n",
    "    for i in range(len(dfs)):\n",
    "        \n",
    "        unique_key = unique_ids_keys[i]\n",
    "        df = dfs[i]\n",
    "        platform_name = dfs_names[i]\n",
    "        all_posts = len(set(df[unique_key].tolist()))\n",
    "      \n",
    "        entities_list = df['entities_list'].tolist()\n",
    "        flat_entities_list = [item for sublist in entities_list for item in sublist]\n",
    "        entities_blacklist = ['LRB', 'RSB', 'RRB', 'Prev', 'PREV >', 'Pres', 'the Best Tl;Dr']\n",
    "        flat_entities_list2 = [x for x in flat_entities_list if x not in entities_blacklist and not x.startswith('####') and not x.startswith('compose?to')]\n",
    "        counter = Counter(flat_entities_list2).most_common(topn)\n",
    "        res = []\n",
    "        top_entities = []\n",
    "        for c in counter:\n",
    "            res.append({'Entity ' + '(' + platform_name + ')' : c[0], 'Percentage (%)': \"%.2f%%\" %(c[1]/all_posts*100)})\n",
    "            top_entities.append(c[0])\n",
    "        top_entities_all.append(top_entities)\n",
    "        output_dfs.append(pd.DataFrame(res))\n",
    "    top_entities_flat = [item for sublist in top_entities_all for item in sublist]\n",
    "    counter_top_entities = Counter(top_entities_flat).most_common(10)\n",
    "    entities_to_return = []\n",
    "    for c in counter_top_entities:\n",
    "        if c[1]>3:\n",
    "            entities_to_return.append(c[0])\n",
    "    return pd.concat(output_dfs, axis=1), entities_to_return\n",
    "    #return pd.concat(output_dfs, axis=1), set(top_entities_all[0]).intersection(*top_entities_all)\n",
    "        \n",
    "top_entities_table, common_entities = find_top_entities(dfs, dfs_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity (Twitter)</th>\n",
       "      <th>Percentage (%)</th>\n",
       "      <th>Entity (Reddit)</th>\n",
       "      <th>Percentage (%)</th>\n",
       "      <th>Entity (The_Donald)</th>\n",
       "      <th>Percentage (%)</th>\n",
       "      <th>Entity (4chan)</th>\n",
       "      <th>Percentage (%)</th>\n",
       "      <th>Entity (Gab)</th>\n",
       "      <th>Percentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump</td>\n",
       "      <td>7.67%</td>\n",
       "      <td>US</td>\n",
       "      <td>11.04%</td>\n",
       "      <td>Trump</td>\n",
       "      <td>9.46%</td>\n",
       "      <td>Trump</td>\n",
       "      <td>13.28%</td>\n",
       "      <td>Trump</td>\n",
       "      <td>2.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>1.46%</td>\n",
       "      <td>Trump</td>\n",
       "      <td>9.03%</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>8.70%</td>\n",
       "      <td>US</td>\n",
       "      <td>10.45%</td>\n",
       "      <td>US</td>\n",
       "      <td>2.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1.26%</td>\n",
       "      <td>Russia</td>\n",
       "      <td>8.28%</td>\n",
       "      <td>Obama</td>\n",
       "      <td>8.05%</td>\n",
       "      <td>UK</td>\n",
       "      <td>8.36%</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>Russian</td>\n",
       "      <td>6.27%</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>7.29%</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.13%</td>\n",
       "      <td>FBI</td>\n",
       "      <td>1.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russia</td>\n",
       "      <td>1.02%</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>5.47%</td>\n",
       "      <td>US</td>\n",
       "      <td>6.66%</td>\n",
       "      <td>Russia</td>\n",
       "      <td>7.75%</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>1.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Obama</td>\n",
       "      <td>1.02%</td>\n",
       "      <td>China</td>\n",
       "      <td>4.03%</td>\n",
       "      <td>CNN</td>\n",
       "      <td>6.50%</td>\n",
       "      <td>EU</td>\n",
       "      <td>6.62%</td>\n",
       "      <td>America</td>\n",
       "      <td>1.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clinton</td>\n",
       "      <td>0.95%</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>3.88%</td>\n",
       "      <td>FBI</td>\n",
       "      <td>5.09%</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>5.76%</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GOP</td>\n",
       "      <td>0.91%</td>\n",
       "      <td>Obama</td>\n",
       "      <td>3.47%</td>\n",
       "      <td>Russia</td>\n",
       "      <td>5.01%</td>\n",
       "      <td>Russian</td>\n",
       "      <td>5.60%</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>1.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UK</td>\n",
       "      <td>0.79%</td>\n",
       "      <td>FBI</td>\n",
       "      <td>3.44%</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>4.62%</td>\n",
       "      <td>Donald J</td>\n",
       "      <td>5.42%</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.68%</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3.30%</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>4.55%</td>\n",
       "      <td>TRUMPTV</td>\n",
       "      <td>5.39%</td>\n",
       "      <td>American</td>\n",
       "      <td>1.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>0.66%</td>\n",
       "      <td>American</td>\n",
       "      <td>2.93%</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>4.54%</td>\n",
       "      <td>American</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>1.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>China</td>\n",
       "      <td>0.64%</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>2.82%</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>4.36%</td>\n",
       "      <td>Jews</td>\n",
       "      <td>5.16%</td>\n",
       "      <td>UK</td>\n",
       "      <td>1.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>America</td>\n",
       "      <td>0.63%</td>\n",
       "      <td>UK</td>\n",
       "      <td>2.72%</td>\n",
       "      <td>American</td>\n",
       "      <td>4.23%</td>\n",
       "      <td>Syria</td>\n",
       "      <td>5.04%</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Russian</td>\n",
       "      <td>0.60%</td>\n",
       "      <td>GOP</td>\n",
       "      <td>2.59%</td>\n",
       "      <td>America</td>\n",
       "      <td>4.16%</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>4.89%</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.57%</td>\n",
       "      <td>Republicans</td>\n",
       "      <td>2.45%</td>\n",
       "      <td>Islam</td>\n",
       "      <td>3.61%</td>\n",
       "      <td>China</td>\n",
       "      <td>4.66%</td>\n",
       "      <td>Americans</td>\n",
       "      <td>0.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FBI</td>\n",
       "      <td>0.56%</td>\n",
       "      <td>White House</td>\n",
       "      <td>2.37%</td>\n",
       "      <td>Soros</td>\n",
       "      <td>3.25%</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>4.44%</td>\n",
       "      <td>California</td>\n",
       "      <td>0.76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Republicans</td>\n",
       "      <td>0.52%</td>\n",
       "      <td>America</td>\n",
       "      <td>2.35%</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>3.17%</td>\n",
       "      <td>Brexit</td>\n",
       "      <td>4.19%</td>\n",
       "      <td>EU</td>\n",
       "      <td>0.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BBC News</td>\n",
       "      <td>0.51%</td>\n",
       "      <td>Putin</td>\n",
       "      <td>2.33%</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>2.85%</td>\n",
       "      <td>Britain</td>\n",
       "      <td>3.78%</td>\n",
       "      <td>Google</td>\n",
       "      <td>0.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Democrats</td>\n",
       "      <td>0.51%</td>\n",
       "      <td>Syria</td>\n",
       "      <td>2.28%</td>\n",
       "      <td>Russian</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>Obama</td>\n",
       "      <td>3.71%</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brexit</td>\n",
       "      <td>0.50%</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>2.23%</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>2.50%</td>\n",
       "      <td>Israeli</td>\n",
       "      <td>3.70%</td>\n",
       "      <td>China</td>\n",
       "      <td>0.72%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Entity (Twitter) Percentage (%)  Entity (Reddit) Percentage (%)  \\\n",
       "0             Trump          7.67%               US         11.04%   \n",
       "1                US          1.46%            Trump          9.03%   \n",
       "2              U.S.          1.26%           Russia          8.28%   \n",
       "3      Donald Trump          1.19%          Russian          6.27%   \n",
       "4            Russia          1.02%             U.S.          5.47%   \n",
       "5             Obama          1.02%            China          4.03%   \n",
       "6           Clinton          0.95%          Clinton          3.88%   \n",
       "7               GOP          0.91%            Obama          3.47%   \n",
       "8                UK          0.79%              FBI          3.44%   \n",
       "9               CNN          0.68%              CNN          3.30%   \n",
       "10  Hillary Clinton          0.66%         American          2.93%   \n",
       "11            China          0.64%        Democrats          2.82%   \n",
       "12          America          0.63%               UK          2.72%   \n",
       "13          Russian          0.60%              GOP          2.59%   \n",
       "14          Hillary          0.57%      Republicans          2.45%   \n",
       "15              FBI          0.56%      White House          2.37%   \n",
       "16      Republicans          0.52%          America          2.35%   \n",
       "17         BBC News          0.51%            Putin          2.33%   \n",
       "18        Democrats          0.51%            Syria          2.28%   \n",
       "19           Brexit          0.50%  Washington Post          2.23%   \n",
       "\n",
       "   Entity (The_Donald) Percentage (%) Entity (4chan) Percentage (%)  \\\n",
       "0                Trump          9.46%          Trump         13.28%   \n",
       "1              Clinton          8.70%             US         10.45%   \n",
       "2                Obama          8.05%             UK          8.36%   \n",
       "3              Hillary          7.29%         Israel          8.13%   \n",
       "4                   US          6.66%         Russia          7.75%   \n",
       "5                  CNN          6.50%             EU          6.62%   \n",
       "6                  FBI          5.09%           U.S.          5.76%   \n",
       "7               Russia          5.01%        Russian          5.60%   \n",
       "8               Muslim          4.62%       Donald J          5.42%   \n",
       "9              Muslims          4.55%        TRUMPTV          5.39%   \n",
       "10     Hillary Clinton          4.54%       American          5.19%   \n",
       "11                U.S.          4.36%           Jews          5.16%   \n",
       "12            American          4.23%          Syria          5.04%   \n",
       "13             America          4.16%         Jewish          4.89%   \n",
       "14               Islam          3.61%          China          4.66%   \n",
       "15               Soros          3.25%        Clinton          4.44%   \n",
       "16           Democrats          3.17%         Brexit          4.19%   \n",
       "17        Donald Trump          2.85%        Britain          3.78%   \n",
       "18             Russian          2.81%          Obama          3.71%   \n",
       "19            Facebook          2.50%        Israeli          3.70%   \n",
       "\n",
       "   Entity (Gab) Percentage (%)  \n",
       "0         Trump          2.98%  \n",
       "1            US          2.16%  \n",
       "2         Obama          2.00%  \n",
       "3           FBI          1.89%  \n",
       "4     Democrats          1.64%  \n",
       "5       America          1.38%  \n",
       "6           CNN          1.34%  \n",
       "7          U.S.          1.21%  \n",
       "8        Russia          1.15%  \n",
       "9      American          1.12%  \n",
       "10       Muslim          1.08%  \n",
       "11           UK          1.06%  \n",
       "12      Russian          0.84%  \n",
       "13     Democrat          0.81%  \n",
       "14    Americans          0.80%  \n",
       "15   California          0.76%  \n",
       "16           EU          0.75%  \n",
       "17       Google          0.74%  \n",
       "18      Hillary          0.72%  \n",
       "19        China          0.72%  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_entities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
